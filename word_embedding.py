import os
import numpy as np
from absl import app
from flags import create_flags, FLAGS, CONST
from document import Document
from util import downloadByURL, han2Jamo
from gensim.models import FastText, fasttext # ë‘˜ì´ ë‹¤ë¦„ ì£¼ì˜!

"""
FastText base word embedding
"""
class WordEmbedding():
    
    def __init__(self):
        return

    def loadDevblogModel(self,
                         embedding_dim,
                         epochs,
                         window,
                         min_count):
        """
        Devblog ë°ì´í„°ë¥¼ ê¸°ë°˜ìœ¼ë¡œ FastText ë‹¨ì–´ ì„ë² ë”© ëª¨ë¸ í•™ìŠµ
        
        - input
        : embedding_dim / int / ë‹¨ì–´ ë²¡í„°í™”ì‹œ ì°¨ì› ìˆ˜
        : epochs / int / í•™ìŠµ íšŸìˆ˜
        : window / int / í•™ìŠµì— ì‚¬ìš©ë  n-gram
        : min_count / int / í•™ìŠµì— ì‚¬ìš©ë  ë‹¨ì–´ì˜ ìµœì†Œ ë“±ì¥íšŸìˆ˜
        
        - return
        : we_model
        """
        model = None
        if not os.path.isfile(CONST.devblog_model_path):
            print('ğŸˆ  í•™ìŠµëœ ë‹¨ì–´ ì„ë² ë”© ëª¨ë¸ì´ ì—†ìŠµë‹ˆë‹¤.')
            dc = Document()
            docs = dc.getDocs(labeled_only=False) # ì „ì²´ ë°ì´í„° ê°€ì ¸ì˜¤ê¸°
            print('ğŸˆ  ë‹¨ì–´ ì„ë² ë”© ëª¨ë¸ í•™ìŠµì„ ì‹œì‘í•©ë‹ˆë‹¤.')
            sentences = docs.text.apply(lambda x: [han2Jamo(s) for s in x.split(' ')])
            model = FastText(size=embedding_dim, window=window, min_count=min_count)
            model.build_vocab(sentences=sentences)
            model.train(sentences=sentences, total_examples=len(sentences), epochs=epochs)
            
            print('ğŸˆ  ë‹¨ì–´ ì„ë² ë”© ëª¨ë¸ì„ ì €ì¥í•©ë‹ˆë‹¤.')
            model.save(CONST.devblog_model_path)
        else:
            model = FastText.load(CONST.devblog_model_path)
        return model
    
    def loadWikiModel(self):
        """
        ìœ„í‚¤ í•œêµ­ì–´ ë°ì´í„°ë¥¼ ê¸°ë°˜ìœ¼ë¡œ FastText ë‹¨ì–´ ì„ë² ë”© ëª¨ë¸ í•™ìŠµ
        : ê¸°ì¡´ í•™ìŠµëœ ëª¨ë¸ì´ ìˆëŠ” ê²½ìš° í•´ë‹¹ ëª¨ë¸ ë°˜í™˜
        : ìœ„í‚¤ í•œêµ­ì–´ ë°ì´í„°(./data/cc.ko.300.bin.gz)ê°€ ì—†ëŠ” ê²½ìš° ë‹¤ìš´ë¡œë“œ
        : ê¸°ì¡´ í•™ìŠµëœ ëª¨ë¸ì´ ì—†ëŠ” ê²½ìš° í•™ìŠµ
        : í•™ìŠµëœ ê²°ê³¼ë¥¼ ./we_modelì— ì €ì¥
        
        - export
        : CONST.wiki_model_path
        """
        model = None
        if not os.path.isfile(CONST.wiki_model_path):
            print('ğŸˆ  í•™ìŠµëœ ë‹¨ì–´ ì„ë² ë”© ëª¨ë¸ì´ ì—†ìŠµë‹ˆë‹¤.')
            
            if not os.path.isfile(CONST.wiki_data_path):
                print('ğŸˆ  ë‹¨ì–´ ì„ë² ë”© ëª¨ë¸ í•™ìŠµì— í•„ìš”í•œ ë°ì´í„°ë¥¼ ë‹¤ìš´ë¡œë“œë¥¼ ì‹œì‘í•©ë‹ˆë‹¤.')
                downloadByURL(CONST.wiki_data_url, CONST.wiki_data_path)
            
            print('ğŸˆ  ë‹¨ì–´ ì„ë² ë”© ëª¨ë¸ í•™ìŠµì„ ì‹œì‘í•©ë‹ˆë‹¤.')
            model = fasttext.load_facebook_model(CONST.wiki_data_path)
            
            print('ğŸˆ  ë‹¨ì–´ ì„ë² ë”© ëª¨ë¸ì„ ì €ì¥í•©ë‹ˆë‹¤.')
            model.save(CONST.wiki_model_path)
        else:
            model = FastText.load(CONST.wiki_model_path)
        
        # print(f'vocab size : {len(model.wv.vocab)}') # 2,000,000
        return model
    
    def getSimilarWords(self, we_model, word, topn=5):
        """
        ìœ ì‚¬ë‹¨ì–´ ì¡°íšŒ
        
        - input
        : we_model / FastText ë‹¨ì–´ ì„ë² ë”© ëª¨ë¸
        : word / str / ìœ ì‚¬ë„ë¥¼ ì¸¡ì •í•˜ë ¤ëŠ” ë‹¨ì–´
        : topn / int / ì¡°íšŒ ê°œìˆ˜
        """
        return we_model.wv.similar_by_word(word, topn)
    
    def embedding(self, we_model, text, embedding_dim=300):
        """
        ì£¼ì–´ì§„ ë¬¸ì¥ì„ ë‹¨ì–´ë³„ë¡œ ë²¡í„°í™”í•œ ë’¤ í‰ê· ê°’ì„ ë¬¸ì¥ì˜ ë²¡í„°ë¡œ ë°˜í™˜
        
        - input
        : we_model / FastText ë‹¨ì–´ ì„ë² ë”© ëª¨ë¸
        : text / str / ë²¡í„°í™”í•˜ë ¤ëŠ” ë¬¸ì¥
        : embedding_dim / int / we_model vectorì˜ ì°¨ì› ìˆ˜ (wiki ê¸°ë°˜ fasttextëŠ” 300ì°¨ì›)
        
        - return
        : nparray / shape = (embedding_dim)
        """
        words = text.split(' ')
        words_num = len(words)
        
        # model dimension (wiki festtextì˜ ê²½ìš° 300)
        vector = np.zeros(embedding_dim)
        for word in words:
            vector += we_model[word]
        return vector/words_num

def main(_):
    we = WordEmbedding()
    we_model = we.loadWikiModel()
    similar_words = we.getSimilarWords(we_model, FLAGS.predict)
    print(similar_words)

if __name__ == '__main__':
    create_flags(True)
    app.run(main)