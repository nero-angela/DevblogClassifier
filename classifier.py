import numpy as np
import matplotlib.pyplot as plt
import tensorflow as tf
from flags import CONST
from sklearn.model_selection import train_test_split
from keras import backend as K
from tensorflow.keras.models import Sequential, model_from_json
from tensorflow.keras.layers import SimpleRNN, Embedding, Dense, Dropout
from tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping

class Classifier():
    
    def __init__(self):
        self.history = None
        
    def _reshape(self, x):
        """
        LSTM ê³„ì—´ì˜ ë ˆì´ì–´ ì‚¬ìš©ì‹œ í•„ìš”í•œ (total, embedding_dim, 1) í˜•íƒœì˜ shapeë¡œ ë³€í™˜
        
        - input
        : x / nparray / ë³€í™˜í•˜ë ¤ëŠ” ë°°ì—´
        
        - return
        : nparray
        """
        return x.reshape(x.shape[0], x.shape[1], 1)
    
    def _dataSeperator(self, data, test_size=0.33):
        """
        ë°ì´í„° ë¶„í• 
        
        - input
        : data / DataFrame / documents.csv ë°ì´í„°
        : test_size / float / ë°ì´í„° ë¶„í•  ë¹„ìœ¨
        
        - return
        : [nparray, nparray, nparray, nparray]
        """
        X_train, X_test, y_train, y_test = train_test_split(data.vector,
                                                            data.label,
                                                            test_size=test_size,
                                                            random_state=321)
        X_train = np.array(X_train.tolist(), dtype=np.float32)
        X_test = np.array(X_test.tolist(), dtype=np.float32)
        y_train = np.array(y_train.tolist(), dtype=np.int32)
        y_test = np.array(y_test.tolist(), dtype=np.int32)
        return X_train, X_test, np.asarray(y_train), np.asarray(y_test)
        
    def train(self,
              data,
              checkpoint_path,
              epochs=75,
              batch_size=100,
              validation_split=0.1,
              verbose=0):
        """
        ëª¨ë¸ í•™ìŠµ
    
        - input
        : data / DataFrame / documents.csv ë°ì´í„°
        : checkpoint_path / str / í•™ìŠµ ì¤‘ê°„ ê²°ê³¼ë¬¼ ì €ì¥ ê²½ë¡œ
        : epochs / int / í•™ìŠµ íšŸìˆ˜
        : batch_size / int / ë°°ì¹˜ ì‚¬ì´ì¦ˆ
        : validation_split / float / validation data ratio
        : verbose / int / 0 = silent, 1 = progress bar, 2 = one line per epoch.

        - return
        : classifier
        
        - export
        : ./model/classifier.json (graph)
        : ./model/classifier.h5 (weights)
        """
        
        # seperate data
        X_train, X_test, y_train, y_test = self._dataSeperator(data)
        
        # model
        K.clear_session()
        model = Sequential()
        model.add(Dense(100, activation='relu', kernel_initializer='he_normal', input_shape=(X_train.shape[1],)))
        model.add(Dense(80, activation='relu', kernel_initializer='he_normal'))
        model.add(Dense(2, activation='softmax'))
        model.compile(optimizer='adam',
                      loss='sparse_categorical_crossentropy',
                      metrics=['acc',
                               self.f1_m,
                               self.precision_m,
                               self.recall_m])
        model.summary()
        
        # checkpoint
        checkpoint = ModelCheckpoint(filepath=checkpoint_path, mode='max', monitor='val_acc', verbose=2, save_best_only=True)
        
        # early stopping
        earlystop_callback = EarlyStopping(monitor='val_acc', min_delta=0.001, patience=3)
        
        self.history = model.fit(X_train,
                            y_train,
                            epochs=epochs,
                            batch_size=batch_size,
                            validation_split=validation_split,
                            callbacks=[checkpoint, earlystop_callback])
        loss, accuracy, f1_score, precision, recall = model.evaluate(X_test, y_test, verbose=verbose)
        print(f'ğŸˆ  loss : {loss}')
        print(f'ğŸˆ  accuracy : {accuracy}')
        print(f'ğŸˆ  f1_score : {f1_score}')
        print(f'ğŸˆ  precision : {precision}')
        print(f'ğŸˆ  recall : {recall}')
        return model

    def predict(self, cf_model, vector, criterion=0.5):
        """
        ê°œë°œê´€ë ¨ ë¬¸ì„œì—¬ë¶€ ì˜ˆì¸¡
        
        - input
        : cf_model / classifier model
        : vector / np.array / embedded vector
        : criterion / float / ê°œë°œê´€ë ¨ ë¬¸ì„œ íŒë‹¨ ê¸°ì¤€
        
        - return
        : boolean / ê°œë°œë¬¸ì„œ ì—¬ë¶€
        : float / 1ì— ê°€ê¹Œìš¸ìˆ˜ë¡ ê°œë°œê´€ë ¨ ë¬¸ì„œ
        """
        confidence = round(cf_model.predict(vector)[0][1], 3)
        is_dev_doc = confidence > criterion
        return is_dev_doc, confidence
        
    def saveModel(self, model, cf_model_path):
        """
        ëª¨ë¸ì˜ parameterì™€ weightsë¥¼ ì €ì¥í•œë‹¤.
        
        - input
        : model / classifier
        : cf_model_path / str / ì €ì¥í•  ê²½ë¡œ
        
        - export
        : ./model/classifier.json / parameter
        : ./model/classifier.h5 / weights
        """
        # save model
        model_json = model.to_json()
        with open(cf_model_path + '.json', "w") as json_file : 
            json_file.write(model_json)
        
        # save weights
        model.save_weights(cf_model_path + '.h5')
        
    def loadModel(self, cf_model_path):
        """
        ëª¨ë¸ì„ ë¶ˆëŸ¬ì˜´

        - input
        : cf_model_path / str / ë¶ˆëŸ¬ì˜¬ ëª¨ë¸
        
        - return
        : classifier
        """
        # load model
        with open(cf_model_path + '.json', "r") as json_file:
            json_model = json_file.read()
        model = model_from_json(json_model)
        
        # load weight
        model.load_weights(cf_model_path + '.h5')
        return model
        
    def showHistory(self):
        """
        train historyë¥¼ ê·¸ë˜í”„ë¡œ ë‚˜íƒ€ëƒ„
        """
        if self.history == None:
            print('ğŸˆ  í•™ìŠµë‚´ì—­ì´ ì—†ìŠµë‹ˆë‹¤.')
            return
        
        fig, loss_ax = plt.subplots()
        acc_ax = loss_ax.twinx()
        acc_ax.plot(self.history.history['acc'], 'b', label='train acc')
        acc_ax.plot(self.history.history['val_acc'], 'g', label='val acc')
        acc_ax.set_ylabel('accuracy')
        acc_ax.legend(loc='upper left')
        plt.show()
    
    def recall_m(self, y_true, y_pred):
        """
        ì¬í˜„ìœ¨(ì‹¤ì œ Trueì¸ ê²ƒ ì¤‘ì—ì„œ ëª¨ë¸ì´ Trueë¼ê³  ì˜ˆì¸¡í•œ ê²ƒì˜ ë¹„ìœ¨) ê³„ì‚°
        
        - input
        : y_true / int / ì •ë‹µ
        : y_pred / int / ëª¨ë¸ ì˜ˆì¸¡ê²°ê³¼
        
        - return
        : float
        """
        true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))
        possible_positives = K.sum(K.round(K.clip(y_true, 0, 1)))
        recall = true_positives / (possible_positives + K.epsilon())
        return recall

    def precision_m(self, y_true, y_pred):
        """
        ì •ë°€ë„(ëª¨ë¸ì´ Trueë¼ê³  ë¶„ë¥˜í•œ ê²ƒ ì¤‘ì—ì„œ ì‹¤ì œ Trueì¸ ê²ƒì˜ ë¹„ìœ¨) ê³„ì‚°
        
        - input
        : y_true / int / ì •ë‹µ
        : y_pred / int / ëª¨ë¸ ì˜ˆì¸¡ê²°ê³¼
        
        - return
        : float
        """
        true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))
        predicted_positives = K.sum(K.round(K.clip(y_pred, 0, 1)))
        precision = true_positives / (predicted_positives + K.epsilon())
        return precision

    def f1_m(self, y_true, y_pred):
        """
        F1 score(Precisionê³¼ Recallì˜ ì¡°í™”í‰ê· ) ê³„ì‚°
        
        - input
        : y_true / int / ì •ë‹µ
        : y_pred / int / ëª¨ë¸ ì˜ˆì¸¡ê²°ê³¼
        
        - return
        : float
        """
        precision = self.precision_m(y_true, y_pred)
        recall = self.recall_m(y_true, y_pred)
        return 2 * ((precision * recall)/(precision + recall + K.epsilon()))
