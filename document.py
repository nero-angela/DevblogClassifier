import os, re, csv, requests, json
import numpy as np
import pandas as pd
from flags import CONST
from enum import Enum
from tqdm import trange
from bs4 import BeautifulSoup
from util import downloadByURL, downloadIfNotExist

class KEYS(Enum):
    # -1 : ì•„ì§ ë¼ë²¨ë§ ì•ˆí•¨ (default)
    # 0  : ê°œë°œê³¼ ê´€ë ¨ì—†ëŠ” ë¬¸ì„œ
    # 1  : ê°œë°œê³¼ ê´€ë ¨ìˆëŠ” ë¬¸ì„œ
    LABEL = 'label'
    
    # TAGS + TITLE + DESC
    TEXT = 'text'
    
    # DATA_URL ê²°ê³¼ íŒŒì‹±ìš© Keys(Beans)
    ID = '_id'
    TITLE = 'title'
    DESC = 'description'
    TAGS = 'tags'
    LINK = 'link'
    
    def getDocKeys():
        """
        awesome-devblog API ìš”ì²­ì‹œ ê°€ì ¸ì˜¤ë ¤ëŠ” ì»¬ëŸ¼
        
        - return
        : list / ì»¬ëŸ¼ëª… ë¦¬ìŠ¤íŠ¸
        """
        return [KEYS.ID.value, KEYS.TITLE.value, KEYS.DESC.value, KEYS.TAGS.value, KEYS.LINK.value]
    
    def getTitleBlackList():
        """
        title ì»¬ëŸ¼ ê¸°ì¤€ ë¸”ë™ë¦¬ìŠ¤íŠ¸
        
        - return
        : list / ë¸”ë™ë¦¬ìŠ¤íŠ¸
        """
        return ['', 'about']
    
    def getTextKeys():
        """
        text ì»¬ëŸ¼ì— ì‚¬ìš©ë˜ëŠ” awesome-devblog ì»¬ëŸ¼
        
        - return
        : list / ì»¬ëŸ¼ëª… ë¦¬ìŠ¤íŠ¸
        """
        return [KEYS.TAGS.value, KEYS.TITLE.value, KEYS.DESC.value]

class Document():
    
    def __init__(self, update=False):

        if update:
            self.updateDocs()
        
    def _getTotal(self):
        """
        awesome-devblogì— ì „ì²´ ë¬¸ì„œ ê°œìˆ˜ ìš”ì²­
        
        - return
        : int / ì „ì²´ ë¬¸ì„œ ê°œìˆ˜
        """
        res = requests.get(CONST.origin_data_url, { 'size': 1 })
        res.raise_for_status()
        doc = res.json()
        return doc['total'][0]['count']

    def _reqDoc(self, page, size, preprocessing=False):
        """
        awesome-devblogì— ë¬¸ì„œ ìš”ì²­
        : KEYSì— ì§€ì •ëœ ì»¬ëŸ¼ë§Œ ê°€ì ¸ì˜´
        
        - input
        : page / int / ìš”ì²­ í˜ì´ì§€(0ë¶€í„° ì‹œì‘)
        : size / int / í•œ ë²ˆì˜ ìš”ì²­ìœ¼ë¡œ ê°€ì ¸ì˜¤ë ¤ëŠ” ë¬¸ì„œ ê°œìˆ˜
        : preprocessing / boolean / ë¬¸ì„œ ì „ì²˜ë¦¬ ì—¬ë¶€
        
        - output
        : DataFrame / DataFrame(response['data'])
        """
        page += 1
        params = {
            'sort': 'date.asc',
            'page': page,
            'size': size
        }
        res = requests.get(CONST.origin_data_url, params)
        res.raise_for_status()
        doc = res.json()
        
        # json to dataframe
        doc = pd.DataFrame(doc['data'], columns=KEYS.getDocKeys())
        
        # add label
        doc.insert(0, KEYS.LABEL.value, -1)
        
        if preprocessing:
            return self.preprocessing(doc)
        else:
            return doc

    def _reqDocs(self, size, start_page=0):
        """
        awesome-devblogì— ì „ì²´ ë¬¸ì„œ ìš”ì²­
        - input
        : size / int / í•œ ë²ˆì˜ ìš”ì²­ìœ¼ë¡œ ê°€ì ¸ì˜¬ ë¬¸ì„œê°œìˆ˜(max 5000)
        : start_page / int / í•´ë‹¹ í˜ì´ì§€ ë¶€í„° ë§ˆì§€ë§‰ í˜ì´ì§€ê¹Œì§€ ì¡°íšŒ
        
        - return
        : DataFrame / ì „ì²˜ë¦¬ëœ ì „ì²´ ë°ì´í„°ë¡œ êµ¬ì„±
        """
        total = self._getTotal()
        if size > CONST.origin_max_req_size: size = CONST.origin_max_req_size
        total_req = round(total/size + 0.5)
        docs = pd.DataFrame()
        for i in trange(start_page, total_req):
            doc = self._reqDoc(i, size)
            if docs.empty:
                docs = doc
            else:
                docs = docs.append(doc)
        return self.preprocessing(docs)
    
    def preprocessing(self, doc, joinTags=True, devblog=False):
        r"""
        ë¬¸ì„œ ì „ì²˜ë¦¬
        : tags / ë°°ì—´ë¡œ ë˜ì–´ìˆìœ¼ë¯€ë¡œ ë„ì–´ì“°ê¸°ë¡œ join
        : title, description, tags / ì˜ì–´, í•œê¸€, ê³µë°±ë§Œ ë‚¨ê¹€
        : html tag ì‚­ì œ
        : \n, \r ì‚­ì œ
        : 2íšŒ ì´ìƒì˜ ê³µë°±ì€ í•˜ë‚˜ë¡œ ì¤„ì…
        : ì˜ì–´ ëŒ€ë¬¸ì ì†Œë¬¸ìë¡œ ë³€í™˜
        : ì•ë’¤ ê³µë°± ì‚­ì œ
        : ë¸”ë™ë¦¬ìŠ¤íŠ¸ ë°ì´í„°(KEYS.getTitleBlackList()) ì œì™¸
        : text / tags + title + description ìˆœì„œë¡œ joinëœ ì»¬ëŸ¼ ìƒì„±
        
        - input
        : doc / DataFrame or str / documents.csv DataFrame
        : joinTags / boolean / tags join ì—¬ë¶€
        
        - return
        : DataFrame / ì „ì²˜ë¦¬ ì™„ë£Œëœ ë°ì´í„°
        """

        if type(doc) == str:
            doc = pd.DataFrame([{
                'title': doc,
                'description': '',
                'tags': []
            }])
        
        # title, description, tags
        def textPreprocessing(x):
            x = BeautifulSoup(str(x), "html.parser").get_text()
            if devblog:
                x = re.sub('[^ã„±-ã…ã…-ã…£_ê°€-í£a-zA-Z\s]', '', x)
            else:
                x = re.sub('[^ê°€-í£a-zA-Z\s]', '', x)
            return x
        
        # all
        def docPreprocessing(x):
            x = re.sub('[\n\r]', '', x)
            x = re.sub('\s{2,}', ' ', x)
            x = x.lower()
            x = x.strip()
            return x
        
        for key in doc.columns:
            if joinTags and KEYS(key) == KEYS.TAGS:
                doc[key] = doc[key].apply(lambda x: ' '.join(x))
            if key in KEYS.getTextKeys():
                doc[key] = doc[key].apply(textPreprocessing)
                
            if key in KEYS.getDocKeys():
                doc[key] = doc[key].apply(docPreprocessing)
            
        # remove blacklist
        doc = doc.drop(doc[doc[KEYS.TITLE.value].isin(KEYS.getTitleBlackList())].index).reset_index()
                        
        # create text column
        join_with = lambda x: ' '.join(x.dropna().astype(str)).strip()
        doc[KEYS.TEXT.value] = doc[KEYS.getTextKeys()].apply(
            join_with,
            axis=1
        )
        return doc
    
    def getDocs(self, labeled_only=True):
        """
        ì „ì²´ ë¬¸ì„œ ì¡°íšŒ
        - input
        : labeled_only / boolean / ë¼ë²¨ë§ ëœ ë°ì´í„°ë§Œ ê°€ì ¸ì˜¬ì§€ ì„ íƒ
        
        - return
        : DataFrame / documents.csv ë°ì´í„°
        """
        downloadIfNotExist(CONST.devblog_data_path, CONST.devblog_data_url)
        data = pd.read_csv(CONST.devblog_data_path, delimiter=',', dtype={KEYS.LABEL.value: np.int64})
        if not labeled_only:
            return data
        else:
            return data.loc[data.label != -1]
    
    def updateDocs(self):
        """
        awesome-devblogì— ìµœì‹  ë¬¸ì„œ ìš”ì²­ ë° documents.csvì— ì¶”ê°€
        : ë°ì´í„°ê°€ ì—†ëŠ” ê²½ìš°, ì „ì²´ ë°ì´í„°ë¥¼ ê°€ì ¸ì˜´
        : ê¸°ì¡´ ë°ì´í„°ê°€ ìˆëŠ” ê²½ìš°, ì—†ëŠ” ë°ì´í„°ë§Œ ì¶”ê°€
       
        - export
        : ./data/documents.csvê°€ ì—†ëŠ” ê²½ìš° ì‹ ê·œ ìƒì„±
        : ./data/documents.csvê°€ ìˆëŠ” ê²½ìš° ì‹ ê·œ ë¬¸ì„œ ì¶”ê°€
        """
        size = CONST.origin_max_req_size
        
        if not os.path.isfile(CONST.devblog_data_path):
            # ë°ì´í„°ê°€ ì—†ëŠ” ê²½ìš°
            docs = self._reqDocs(size)
            docs.to_csv(CONST.devblog_data_path, sep=",", index=False)
        else:
            # ê¸°ì¡´ ë°ì´í„°ê°€ ìˆëŠ” ê²½ìš°
            docs = pd.read_csv(CONST.devblog_data_path, delimiter=',')
            total = self._getTotal()
            total_docs = len(docs)
            new_docs = self._reqDocs(size, total_docs // size)
            
            # _idê°€ ê¸°ì¡´ ë°ì´í„°ì— ì¡´ì¬í•˜ì§€ ì•ŠëŠ” ê²½ìš°ì—ë§Œ ì¶”ê°€
            docs = docs.append(new_docs[~new_docs[KEYS.ID.value].isin(docs[KEYS.ID.value])])
            docs.to_csv(CONST.devblog_data_path, sep=",", index=False)
            
            if total_docs == len(docs):
                print('ğŸˆ ë¬¸ì„œê°€ ìµœì‹  ìƒíƒœì…ë‹ˆë‹¤.')
            else:
                print(f'ğŸˆ ì‹ ê·œ ë¬¸ì„œ {len(docs) - total_docs}ê°œ ì¶”ê°€')
    
    def syncDocLabel(self, old_document_path, sep, override=False):
        """
        ê¸°ì¡´ ë¼ë²¨ë§í•œ ë°ì´í„°ë¥¼ ì‹ ê·œ ë¬¸ì„œì— ë°˜ì˜
        : title, link ê¸°ì¤€ìœ¼ë¡œ ì¼ì¹˜í•˜ëŠ” ë¬¸ì„œ ê²€ìƒ‰
        
        - input
        : old_document_path / str / ê¸°ì¡´ ë¼ë²¨ë§í•œ ë°ì´í„° ê²½ë¡œ
        : sep / str / csv delimiter
        : override / boolean / ê¸°ì¡´ ë¼ë²¨ë§ì´ ë°˜ì˜ëœ ê²°ê³¼ë¥¼ ./data/documents.csvë¡œ ì €ì¥ì—¬ë¶€
        
        - export
        : ./data/documents.csv
        """
        
        document = pd.read_csv(CONST.devblog_data_path, delimiter=',')
        old_document = pd.read_csv(old_document_path, delimiter=sep)
        self.preprocessing(old_document, joinTags=False)
        for index, row in old_document.iterrows():
            link = row.link
            title = row.title
            label = int(row.label)
            if not len(document.loc[document.title.str.strip() == title.strip()]) and not len(document.loc[document.link == link]):
                print(f'ğŸˆ not found : {row.title}')
            elif len(document.loc[document.title.str.strip() == title.strip()]):
                document.loc[document.title.str.strip() == title.strip(), KEYS.LABEL.value] = label
            elif len(document.loc[document.link == link]):
                document.loc[document.link == link, KEYS.LABEL.value] = label
        
        # save synchronized document
        if override:
            document.to_csv(CONST.devblog_data_path, sep=",", index=False)
        print('ğŸˆ done')
